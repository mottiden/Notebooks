{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Models\n",
    "\n",
    "Vector models are awesome. Period. Why? Because they allow fast searches in multi-dimensional spaces. \n",
    "\n",
    "\n",
    "Vectors  represent a point in a multi dimensional space.\n",
    "\n",
    "* Carthesian example 2 dimensional space.\n",
    "\n",
    "* Example x=[1,0.5,2] in a three dimensional case. \n",
    "\n",
    "* Images are high dimensional\n",
    "\n",
    "\n",
    "### An obvious solution for high dimension problems\n",
    "\n",
    "Discovering high level structure in data can be helped when the dimensions are reduced. This makes it easy to compute similarity and search for nearest neighbors.\n",
    "\n",
    "\n",
    "### How do you search for neighbors in vector spaces?\n",
    "\n",
    "#### k-NN\n",
    "\n",
    "The idea is to find the closest k-neighbour. \n",
    "\n",
    "Examples. You have some points and you want to find the closest \n",
    "\n",
    "It is difficult to represent this in higher dimensions because of the limitations of the graphical interface, but keep in mind that these are fast methods for regression and classification in high dimensions.\n",
    "\n",
    "MINST example\n",
    "\n",
    "\n",
    "\n",
    "#### ANN\n",
    "A different variation of the k-NN algorithm is the Approximate NN. Used to cope with more dimensions because it is faster. \n",
    "\n",
    "* Split data in two parts picking two points and dividing by the hyperplane\n",
    "  * Split again until k-items in each part\n",
    "  * End up building a binary tree\n",
    "  * Takes (n/k) memory instead of n\n",
    "* Searching\n",
    "  * The closest points might be on different sides of the split\n",
    "  * Then go both sides of the split with Priority queue\n",
    "  * Build forests with many random trees\n",
    "  * Search multiple trees at the same time with the priority queue - best splits with biggest distance - until found k items\n",
    "  * Take union and remove duplicates\n",
    "  * Compute the distance of the remaining items\n",
    "  * Return the nearest items\n",
    "  \n",
    "#### The curse of dimensionality\n",
    "Increasing the dimensions the distances to the different points becomes the same. \n",
    "\n",
    "\n",
    "Vector methods for text:\n",
    "* Latent Semantic Analysis (1988)\n",
    "* Probabilistic Latent Semantic Analysis (2000)\n",
    "* Semantic Hashing (2007)\n",
    "* word2vec (2013), RNN, LSTM\n",
    "\n",
    "\n",
    "References:\n",
    "[1] Erik Approximate nearest neighbors and vector models, introduction to Annoy. https://www.youtube.com/watch?v=QkCCyLW0ehU\n",
    "[2]\n",
    "\n",
    "\n",
    "NOTE: Reduce everything to trees and hash tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
