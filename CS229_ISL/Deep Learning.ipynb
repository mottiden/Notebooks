{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Andrew Ng overview from CS229 - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a new airplane engine that comes from the assembly line, can we classify whether it is an anamoly?\n",
      "\n",
      "History of AI:\n",
      "    \n",
      "* Good product.\n",
      "* Many users.\n",
      "* Large amount of data, which feeds back to product.\n",
      "\n",
      "The AI piece was always missing:\n",
      "\n",
      "* Performance of most learning algorithms usually hit a plateu.\n",
      "* These algorithms usually did not have (1) high VC dimention or (2) high computational efficincy.\n",
      "* For example, SVMs can address (1) but can't well (e.g., to Terabytes of data with a large number of support vevtors) with (2). \n",
      "\n",
      "Features are required to re-cast hard (speech, vision) problems:\n",
      "\n",
      "* For these, people have built complex pipelines to capture features for perception.\n",
      "* There is a large literature in NLP to come up with features (e.g., parsers to spit out all Nouns).\n",
      "* If you come up with better features, then the learning should work better.\n",
      "\n",
      "Deep learning is inspired by neural networks:\n",
      "\n",
      "* Each node in the network is logistic regression.\n",
      "* We can compute complex non-linear representations of the network.\n",
      "\n",
      "Value in deep learning has, recently, been applied to supervised learning:\n",
      "\n",
      "* Has, to date, been on labeled data.\n",
      "* It is very effective at learning a funtion that maps from $x$ to $y$.\n",
      "* It should soon be applied to unsupervised learning.\n",
      "\n",
      "With Big Data:\n",
      "\n",
      "* Usually in the high bias, low variance regime.\n",
      "* This is because there is a large amount of complex data, so underfitting is more likely. \n",
      "* In turn, building a larger model can help. \n",
      "* However, more learning models begin to suffer from poor performance (especially on very large data). \n",
      "\n",
      "Google Brain used deep learning for speech recognition:\n",
      "\n",
      "* The goal was to build huge neural networks (16,000 CPUs). \n",
      "* This went to 1 billion connections.\n",
      "* Speech recognition has used this, as there are thousands of hours of speech data with associated transcripts.\n",
      "* Giant neural networks were used to soak up these massive datasets.\n",
      "\n",
      "Neuroscientsts beleive that infants learn from unsupervised (unlabeled) data:\n",
      "\n",
      "* When human brain processes images, it looks for lines (edges).\n",
      "\n",
      "Neuroscientists developed an algorithm called \"Sparse Coding\" that can be used to learn:\n",
      "\n",
      "* This method \"invents\" edge detection.\n",
      "* Sparse coding is very similar to ICA.\n",
      "* They used a billion parameter neural network, and made it watch YouTube for a week.\n",
      "* It can learn to detect human faces; cat detector. \n",
      "* It discovered a cat from unlabeled data.\n",
      "\n",
      "Implmentation has a multi-tier \"depth\" (from one to over 10 layers of representation) to learn from un-labeled data:\n",
      "\n",
      "* Group together pixels.\n",
      "* Group together edges to make detectors of object parts.\n",
      "* Then part detectors can be used to detect faces or cats.\n",
      "\n",
      "Updated CPU architecture for Deep Learning using CUDA/GPUs:\n",
      "\n",
      "* GPU architecture for deep learning; very good for scaling because neural networks parallelize well. \n",
      "\n",
      "Computer vision:\n",
      "\n",
      "* Moved from pipeline with feature extraction (SIFT) and prediction (SVM) to, soon, soley deep learning.\n",
      "\n",
      "Speech recognition:\n",
      "\n",
      "* Moved from pipeline with acustic model (GMM), language model (ngram), and inference (HMM) to deep learning.\n",
      "* Wants speech recognition for mobile to be the interface.\n",
      "\n",
      "Notes:\n",
      "\n",
      "* deeplearning.stanford.edu\n",
      "* https://www.youtube.com/watch?v=vShMxxqtDDs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Integrate notes from Karpathy's class - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://cs231n.github.io/neural-networks-1/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://cs231n.github.io/neural-networks-2/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://cs231n.github.io/neural-networks-3/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://cs231n.github.io/convolutional-networks/"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Integrate notes from here - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://www.iro.umontreal.ca/~bengioy/dlbook/intro.html"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}