{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Naive Bayes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Preface - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are notes I wrote for myself while studying for PhD qualifying exams at Stanford. \n",
      "\n",
      "I wanted to published them so that others could benefit.\n",
      "\n",
      "They draw from:\n",
      "\n",
      "* CS229, an excellent machine learning class at Stanford taught by Andrew Ng.\n",
      "* http://nbviewer.ipython.org/github/gmonce/scikit-learn-book/blob/master/Chapter%202%20-%20Supervised%20Learning%20-%20Text%20Classification%20with%20Naive%20Bayes.ipynb\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quick cheatsheet - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* Classification: It will compute a class label.\n",
      "* Generative: It will try to learn the features, given the label, $P(x \\mid y)$.\n",
      "\n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* $P(X \\mid y) = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "* $\\phi_{i|y=1} = p(x_i = 1|y = 1)$\n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "  \n",
      "* For GDA, we defined a multinomial Gaussian such that each combination of features had a unique probability assigned to it!\n",
      "* But, that doesn't scale well to cases of many features (e.g., text classification with a large dictionary).\n",
      "* Here, we assume the occournace of every feature is conditionally independent given $y$.\n",
      "* That is the probability of observing the words \"buy\" and \"Viagra\" in a spam e-mail are independent.\n",
      "\n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "\n",
      "* $ L = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "\n",
      "$\\textbf{Strategies for training our model parameters} -$  \n",
      "\n",
      "(1) Analytical approaches to find model parameters:\n",
      "\n",
      "* In this case, took the derivative of the likelihood function with respect to each and set equal to zero.\n",
      "* This allowed us to derive formulas for the maximum likelihood estimates of each parameter (e.g., $\\phi_{i|y=1}$).\n",
      "\n",
      "$\\textbf{Implementing the model} -$\n",
      "\n",
      "* This is the same as GDA!\n",
      "* We train maximum likelihood estimates of each parameter on data for each class.\n",
      "* For test data, we then compute the posterior probability that it belongs to each class, using Bayes rule:\n",
      "* $ P(Y=1|X) = \\frac{P(X|y=1)P(y=1)}{P(X)} $\n",
      "* To assign the test example, $X$, we simply compute the posterior probability for each of the classes and pick the better one!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Generative algorithms - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall what we did with linear discriminant analysis:\n",
      "\n",
      "* We assumed Gaussian density funtion for our features with respect to each class.\n",
      "* With the training data, we then parameterized Gaussian densities (using likelihood maximization).\n",
      "* For each test data, we computed posterior probabilities for each class and simply chose the best one.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "An approach without the multinomial distribution - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But, we assumed a multinomial distribution to model the probability of observing different features combinations for each class.\n",
      "\n",
      "Consider problems like text classification in which the feature space (e.g., a large dictionary of words) is huge.\n",
      "\n",
      "Given a class label (e.g., spam), it would be challenging to model all combinations using the multinomial distribution.\n",
      "\n",
      "---\n",
      "\n",
      "Consider another way to do text classification.\n",
      "\n",
      "We convert an e-mail into a bit vector where each position specifies a word in a large (e.g., 10,000 word) dictionary.\n",
      "\n",
      "Before, the multinomial distribution modeled a unique probability associated with each feature $combination$!\n",
      "\n",
      "Now, we just assume (Naive Bayes) that each word, $x_i$, is conditionally independent, given y:\n",
      "\n",
      "$P(X \\mid y) = P(x_1,x_2,x_3 \\mid y) = P(x_1 \\mid y)$$P(x_2 \\mid y)$$P(x_3 \\mid y) = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "\n",
      "When m indexes over the dictionary, the probability of each word appearing is independent of seeing any other.\n",
      "\n",
      "Using Bayes rule, we then compute the class for any given e-mail:\n",
      "\n",
      "$$ P(Y=1\\mid X) = \\frac{P(X \\mid y=1)P(y=1)}{P(X)} $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Learning strategy - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As before, we define a likelihood funtion:\n",
      "\n",
      "* $ L = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "\n",
      "As with GDA, we use analytical approaches to find model parameters:\n",
      "\n",
      "* In this case, took the derivative of the likelihood function with respect to each and set equal to zero.\n",
      "* This allowed us to derive formulas for the maximum likelihood estimates of each parameter (e.g., $\\phi_{i|y=1}$)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Implementation - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Furthermore, in CS229 we showed that we can compute the joint likelihood, which is the product of predicted value for our class. \n",
      "\n",
      "We can take the derivitive, set it to zero (to find the maximum), and then find the expected parameters.\n",
      "\n",
      "$$ l() = \\sum_{i=1}^m log (P(x \\mid y)) p(y)$$\n",
      "\n",
      "This results in very intuitive estimates for the model parameters:\n",
      "\n",
      "For example $P(j \\mid y=0)$ is simply the ocourance of the given word divided by the total number of e-mails in the class $y=0$.\n",
      "\n",
      "For test data, we then compute the posterior probability that it belongs to each class, using Bayes rule, $ P(Y=1|X) = \\frac{P(X|y=1)P(y=1)}{P(X)} $.\n",
      "\n",
      "To assign the test example, $X$, we simply compute the posterior probability for each of the classes and pick the better one!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn as sk\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's pull some news group data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "news = fetch_20newsgroups(subset='all')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "['DESCR', 'data', 'target', 'target_names', 'filenames']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A post."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print news.data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
        "Subject: Pens fans reactions\n",
        "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
        "Lines: 12\n",
        "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
        "\n",
        "\n",
        "\n",
        "I am sure some bashers of Pens fans are pretty confused about the lack\n",
        "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
        "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
        "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
        "are killing those Devils worse than I thought. Jagr just showed you why\n",
        "he is much better than his regular season stats. He is also a lot\n",
        "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
        "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
        "regular season game.          PENS RULE!!!\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Category for the post."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print news.target[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news.target_names[news.target[0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "'rec.sport.hockey'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, first split the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "P = 0.8\n",
      "split_size = int(len(news.data)*P)\n",
      "X_train = news.data[:split_size]\n",
      "X_test = news.data[split_size:]\n",
      "y_train = news.target[:split_size]\n",
      "y_test = news.target[split_size:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, recall that we want to run cross-validation on our data.\n",
      "\n",
      "A model is trained using $k-1$ of the folds as training data.\n",
      "\n",
      "The resulting model is validated on the remaining part of the data.\n",
      "\n",
      "It is used as a test set to compute a performance measure such as accuracy.\n",
      "\n",
      "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset:\n",
      "\n",
      "http://scikit-learn.org/stable/modules/cross_validation.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score, KFold\n",
      "from scipy.stats import sem\n",
      "\n",
      "def evaluate_cross_validation(clf, X, y, K):\n",
      "    # K-fold cross validation iterator of k=5 folds\n",
      "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
      "    # Score used is the one returned by score method of the estimator (accuracy)\n",
      "    scores = cross_val_score(clf, X, y, cv=cv)\n",
      "    print scores\n",
      "    print (\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, let's build a pipeline for this.\n",
      "\n",
      "Recall that we have to convert works into token counts: \n",
      "\n",
      "* The algorithms can work only on numeric data, so our next step will be to convert our text-based dataset to a numeric dataset.\n",
      "* We use `CountVectorizer()`\n",
      "\n",
      "`CountVectorizer` creates a dictionary of words from the text corpus. \n",
      "\n",
      "Then, each instance is converted to a vector of numeric features where each element will be the count of the number of times a particular word appears in the document. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "clf_1 = Pipeline([\n",
      "    ('vect', CountVectorizer()),\n",
      "    ('clf', MultinomialNB()),\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's run the pipeline."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_cross_validation(clf_1,news.data,news.target,5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.85782493  0.85725657  0.84664367  0.85911382  0.8458477 ]\n",
        "Mean score: 0.853 (+/-0.003)\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can then tune the model.\n",
      "\n",
      "There's some nice points in this notebook:\n",
      "\n",
      "http://nbviewer.ipython.org/github/gmonce/scikit-learn-book/blob/master/Chapter%202%20-%20Supervised%20Learning%20-%20Text%20Classification%20with%20Naive%20Bayes.ipynb"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
      "    \n",
      "    clf.fit(X_train, y_train)\n",
      "    \n",
      "    print \"Accuracy on training set:\"\n",
      "    print clf.score(X_train, y_train)\n",
      "    print \"Accuracy on testing set:\"\n",
      "    print clf.score(X_test, y_test)\n",
      "    \n",
      "    y_pred = clf.predict(X_test)\n",
      "    \n",
      "    print \"Classification Report:\"\n",
      "    print metrics.classification_report(y_test, y_pred)\n",
      "    print \"Confusion Matrix:\"\n",
      "    print metrics.confusion_matrix(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_and_evaluate(clf_1, X_train, X_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy on training set:\n",
        "0.926638365614"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on testing set:\n",
        "0.844827586207"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Classification Report:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.89      0.86      0.88       175\n",
        "          1       0.62      0.86      0.72       199\n",
        "          2       0.93      0.13      0.22       221\n",
        "          3       0.59      0.85      0.70       179\n",
        "          4       0.88      0.88      0.88       177\n",
        "          5       0.78      0.91      0.84       179\n",
        "          6       0.88      0.68      0.77       205\n",
        "          7       0.89      0.89      0.89       228\n",
        "          8       0.94      0.94      0.94       183\n",
        "          9       0.96      0.93      0.95       197\n",
        "         10       0.96      0.99      0.98       204\n",
        "         11       0.90      0.99      0.94       218\n",
        "         12       0.87      0.88      0.87       172\n",
        "         13       0.91      0.90      0.91       200\n",
        "         14       0.93      0.94      0.93       198\n",
        "         15       0.81      0.95      0.87       191\n",
        "         16       0.80      0.95      0.87       173\n",
        "         17       0.90      0.99      0.94       184\n",
        "         18       0.83      0.84      0.84       172\n",
        "         19       0.94      0.54      0.69       115\n",
        "\n",
        "avg / total       0.86      0.84      0.83      3770\n",
        "\n",
        "Confusion Matrix:\n",
        "[[151   1   0   0   0   0   0   0   0   0   1   0   1   1   0  14   1   2\n",
        "    1   2]\n",
        " [  0 171   0   6   0   7   4   0   0   1   0   3   0   1   1   0   1   0\n",
        "    4   0]\n",
        " [  0  53  28  76  10  32   0   2   1   2   0   6   2   0   1   2   0   0\n",
        "    5   1]\n",
        " [  0   6   1 152   3   5   3   0   0   1   0   3   3   0   1   0   1   0\n",
        "    0   0]\n",
        " [  0   6   0   5 155   1   2   0   0   0   0   3   4   1   0   0   0   0\n",
        "    0   0]\n",
        " [  0  13   0   1   1 163   0   1   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   8   0   9   4   0 139  15   1   1   2   4   7   3   1   1   4   3\n",
        "    3   0]\n",
        " [  1   1   0   1   0   1   5 202   2   1   0   0   2   2   0   0   4   1\n",
        "    5   0]\n",
        " [  0   0   0   1   0   0   2   2 172   0   0   1   0   1   1   0   1   1\n",
        "    1   0]\n",
        " [  0   0   0   0   1   0   0   2   3 183   3   0   2   1   0   1   0   1\n",
        "    0   0]\n",
        " [  0   1   0   0   0   0   0   0   0   0 202   0   0   0   0   0   0   0\n",
        "    1   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0 215   0   0   0   0   1   1\n",
        "    1   0]\n",
        " [  0   5   0   2   2   0   0   0   0   0   1   1 151   3   6   0   0   0\n",
        "    1   0]\n",
        " [  0   2   0   0   0   0   0   1   1   0   1   0   2 180   3   4   4   1\n",
        "    1   0]\n",
        " [  0   6   0   0   0   0   2   0   0   0   0   0   0   1 186   0   1   1\n",
        "    1   0]\n",
        " [  1   1   0   3   0   0   0   1   0   0   0   1   0   1   0 182   0   1\n",
        "    0   0]\n",
        " [  0   0   1   0   0   0   1   0   2   0   0   2   0   0   0   0 164   2\n",
        "    1   0]\n",
        " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 182\n",
        "    1   0]\n",
        " [  1   0   0   1   0   0   0   1   1   0   0   1   0   1   0   2  13   5\n",
        "  145   1]\n",
        " [ 15   0   0   0   0   1   0   0   0   1   0   0   0   1   1  19  10   1\n",
        "    4  62]]\n"
       ]
      }
     ],
     "prompt_number": 23
    }
   ],
   "metadata": {}
  }
 ]
}